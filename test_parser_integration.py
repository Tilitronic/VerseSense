#!/usr/bin/env python3
"""
Test the integrated parser with detailed stage-by-stage logging.
Tests: TXT parsing â†’ Trie parsing â†’ spaCy standardization
"""

from pathlib import Path
import sys
import time

# Add src to path
sys.path.insert(0, str(Path(__file__).parent))

from src.stress_db_generator.parser import StressDictionary


def print_header(title, char="="):
    """Print a formatted section header"""
    print("\n" + char * 80)
    print(title)
    print(char * 80)


def print_stage(stage_num, stage_name):
    """Print stage indicator"""
    print(f"\n{'â–ˆ' * 80}")
    print(f"STAGE {stage_num}: {stage_name}")
    print(f"{'â–ˆ' * 80}\n")


def test_word_lookup(dictionary, word, stage_name):
    """Test a single word lookup and display results"""
    print(f"\n  Word: '{word}'")
    forms = dictionary.lookup(word)
    
    if not forms:
        print(f"    âŒ NOT FOUND in {stage_name}")
        return
    
    print(f"    âœ“ Found {len(forms)} form(s) in {stage_name}:")
    for i, form in enumerate(forms, 1):
        print(f"      [{i}] Stress: {form.stress_variants}", end="")
        if form.pos:
            pos_str = ",".join(form.pos)
            print(f" | POS: [{pos_str}]", end="")
        if form.feats:
            # Display features with list values
            feats_parts = []
            for k, v in sorted(form.feats.items()):
                v_str = ",".join(v) if len(v) > 1 else v[0]
                feats_parts.append(f"{k}={v_str}")
            feats_str = "|".join(feats_parts)
            print(f" | Feats: {feats_str}", end="")
        print()


def test_lookups_batch(dictionary, test_words, stage_name):
    """Run batch lookup tests"""
    print_header(f"Test Lookups - {stage_name}", "-")
    for word in test_words:
        test_word_lookup(dictionary, word, stage_name)


def show_statistics(dictionary, stage_name):
    """Display dictionary statistics"""
    print_header(f"Statistics - {stage_name}", "-")
    stats = dictionary.get_statistics()
    print(f"  ðŸ“Š Total unique words:            {stats['total_unique_words']:>12,}")
    print(f"  ðŸ“Š Total word forms:              {stats['total_forms']:>12,}")
    print(f"  ðŸ“Š Heteronyms (multi-stress):     {stats['words_with_unique_stress']:>12,}")
    print(f"  ðŸ“Š Avg forms per word:            {stats['avg_forms_per_word']:>12}")
    
    # Calculate morphology coverage
    words_with_morph = sum(1 for forms in dictionary.words.values() 
                          if any(f.pos or f.feats for f in forms))
    morph_percent = (words_with_morph / len(dictionary.words) * 100) if dictionary.words else 0
    print(f"  ðŸ“Š Words with morphology:         {words_with_morph:>12,} ({morph_percent:.1f}%)")


def show_sample_heteronyms(dictionary, count=5):
    """Show sample heteronyms with full details"""
    print_header("Sample Heteronyms (Multi-stress words)", "-")
    heteronyms = dictionary.get_words_with_unique_stress()
    print(f"  Displaying {count} of {len(heteronyms):,} total heteronyms:\n")
    
    for word, forms in heteronyms[:count]:
        print(f"  ðŸ“– '{word}' â€” {len(forms)} distinct forms:")
        for idx, form in enumerate(forms, 1):
            stress_str = ", ".join(str(i) for i in form.stress_variants)
            morph_parts = []
            if form.pos:
                pos_str = ",".join(form.pos)
                morph_parts.append(f"POS=[{pos_str}]")
            if form.feats:
                feats_parts = []
                for k, v in sorted(form.feats.items()):
                    v_str = ",".join(v) if len(v) > 1 else v[0]
                    feats_parts.append(f"{k}={v_str}")
                morph_parts.append("|".join(feats_parts))
            morph_display = " [" + ", ".join(morph_parts) + "]" if morph_parts else ""
            print(f"      [{idx}] Stress@vowel: {stress_str}{morph_display}")


def main():
    print_header("UKRAINIAN STRESS DICTIONARY - MULTI-STAGE INTEGRATION TEST")
    print("Testing pipeline: TXT â†’ Trie â†’ spaCy Standardization")
    
    # Test words for consistent validation across stages
    test_words = ["Ð°Ñ‚Ð»Ð°Ñ", "Ð±Ð»Ð¾Ñ…Ð¸", "Ð¿Ð¾Ð¼Ð¸Ð»ÐºÐ°", "ÐºÐ¾Ñ€Ð¸Ð´Ð¾Ñ€", "Ð·Ð°Ð¼Ð¾Ðº"]
    
    # ========================================================================
    # STAGE 1: TXT File Parsing (if exists)
    # ========================================================================
    print_stage(1, "TXT File Parsing (Optional)")
    dictionary = StressDictionary()
    
    txt_path = Path("src/stress_db_generator/raw_data/sample_stress_dict.txt")
    if txt_path.exists():
        print(f"ðŸ“‚ Loading: {txt_path}")
        start = time.time()
        dictionary.parse_file(txt_path)
        elapsed = time.time() - start
        print(f"â±ï¸  Parsing completed in {elapsed:.2f}s")
        
        show_statistics(dictionary, "After TXT Parsing")
        test_lookups_batch(dictionary, test_words, "TXT Dictionary")
    else:
        print(f"â„¹ï¸  No TXT file found at {txt_path} - skipping this stage")
    
    # ========================================================================
    # STAGE 2: Trie File Parsing
    # ========================================================================
    print_stage(2, "Trie File Parsing & Merging")
    
    trie_path = Path("src/stress_db_generator/raw_data/stress.trie")
    if not trie_path.exists():
        print(f"âŒ ERROR: Trie file not found at {trie_path}")
        return 1
    
    print(f"ðŸ“‚ Loading: {trie_path}")
    print("â³ Processing ~2.9M entries (this takes ~30-60 seconds)...")
    
    start = time.time()
    try:
        dictionary.parse_trie(trie_path)
        elapsed = time.time() - start
        print(f"â±ï¸  Parsing completed in {elapsed:.2f}s ({dictionary.words.__len__():,} words/sec)")
        
        show_statistics(dictionary, "After Trie Parsing")
        test_lookups_batch(dictionary, test_words, "Trie Dictionary")
        
    except Exception as e:
        print(f"\nâŒ ERROR during trie parsing: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    # ========================================================================
    # STAGE 3: spaCy Standardization Verification
    # ========================================================================
    print_stage(3, "spaCy/Universal Dependencies Format Verification")
    
    print("âœ“ Verifying data structure conforms to spaCy standards...")
    print("\nspaCy-compatible structure requirements:")
    print("  â€¢ pos: Universal POS tags (NOUN, VERB, ADJ, etc.)")
    print("  â€¢ feats: Morphological features dict (Case, Number, Gender, etc.)")
    print("  â€¢ Features alphabetically sorted")
    print("  â€¢ Follows Universal Dependencies (UD) annotation scheme\n")
    
    # Validate structure
    sample_words_with_morph = []
    for word, forms in dictionary.words.items():
        if any(f.pos and f.feats for f in forms):
            sample_words_with_morph.append((word, forms))
            if len(sample_words_with_morph) >= 3:
                break
    
    if sample_words_with_morph:
        print("âœ“ Sample words with full spaCy-compatible annotation:\n")
        for word, forms in sample_words_with_morph:
            print(f"  '{word}':")
            for form in forms[:2]:  # Show first 2 forms
                print(f"    â€¢ to_dict(): {form.to_dict()}")
                print(f"    â€¢ to_spacy_format(): \"{form.to_spacy_format()}\"")
    
    test_lookups_batch(dictionary, test_words, "spaCy-standardized Dictionary")
    
    # ========================================================================
    # FINAL SUMMARY
    # ========================================================================
    print_header("FINAL SUMMARY")
    
    show_statistics(dictionary, "Complete Dictionary")
    show_sample_heteronyms(dictionary, count=8)
    
    print("\n" + "=" * 80)
    print("âœ… ALL STAGES COMPLETED SUCCESSFULLY")
    print("=" * 80)
    print(f"\nðŸ“š Dictionary ready with {len(dictionary.words):,} words")
    print(f"ðŸ“‹ spaCy-compatible format: âœ“")
    print(f"ðŸŽ¯ Ready for NLP pipeline integration\n")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())
